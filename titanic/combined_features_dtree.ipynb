{
 "metadata": {
  "name": "",
  "signature": "sha256:692df15d3ebe7e8f49528cb4f95b48c9e50996157115fc9dda36d10c7103af81"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For simplicity, we are going to extract all variables as discrete ones. So we will be binning continuous variables."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Common Functions ##"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "''' THE DATA \n",
      "\n",
      "top 2 lines of train.csv\n",
      "\n",
      "    PassengerId,Survived,Pclass,Name,Sex,Age,SibSp,Parch,Ticket,Fare,Cabin,Embarked\n",
      "    1,0,3,\"Braund, Mr. Owen Harris\",male,22,1,0,A/5 21171,7.25,,S\n",
      "\n",
      "top 2 lines of test.csv\n",
      "\n",
      "    PassengerId,Pclass,Name,Sex,Age,SibSp,Parch,Ticket,Fare,Cabin,Embarked\n",
      "    892,3,\"Kelly, Mr. James\",male,34.5,0,0,330911,7.8292,,Q\n",
      "\n",
      "top 2 lines of sample output - myfirstforest.csv\n",
      "\n",
      "    PassengerId,Survived\n",
      "    892,0\n",
      "\n",
      "'''\n",
      "None"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import csv\n",
      "\n",
      "def bin_fare(fare):\n",
      "    \"\"\"\n",
      "        1 = under 25 (default)\n",
      "        2 = 25-70\n",
      "        3 = over 70\n",
      "    \"\"\"\n",
      "    if not fare:\n",
      "        return 1\n",
      "    fare = float(fare)\n",
      "    if 70 <= fare:\n",
      "        return 3\n",
      "    if 25 <= fare <= 70:\n",
      "        return 2\n",
      "    return 1\n",
      "\n",
      "def bin_age(age):\n",
      "    \"\"\" \n",
      "        1 = under 18\n",
      "        2 = 18-35 (default)\n",
      "        3 = 35-55\n",
      "        4 = over 55\n",
      "    \"\"\"\n",
      "    if not age:\n",
      "        return 2\n",
      "    age = float(age)\n",
      "    if age <= 18:\n",
      "        return 1\n",
      "    if 35 <= age <= 55:\n",
      "        return 3\n",
      "    if 55 <= age: \n",
      "        return 4\n",
      "    return 2\n",
      "    \n",
      "\n",
      "def load_data(path, is_training):\n",
      "    \"\"\"\n",
      "    is_training = \n",
      "        True:  Return raw Training Set and Labels\n",
      "        False: Return raw Test Set\n",
      "    \"\"\"\n",
      "    with open(path) as csvfile:\n",
      "        training_file = csv.DictReader(csvfile)\n",
      "        ids = []\n",
      "        data = []\n",
      "        labels = []\n",
      "        for row in training_file:\n",
      "            if is_training:\n",
      "                labels.append(row['Survived'])\n",
      "                del row['Survived'] # we can't train on survived!\n",
      "            else:\n",
      "                ids.append(row['PassengerId'])\n",
      "            del row['PassengerId'] # we can't train on PassengerId!\n",
      "            del row['Name'] # for simplicity, we won't try to analyze name, ticket, or cabin\n",
      "            del row['Ticket']\n",
      "            del row['Cabin']\n",
      "            row['Fare'] = bin_fare(row['Fare'])\n",
      "            row['Age'] = bin_age(row['Age'])\n",
      "            row['SibSp'] = int(row['SibSp'])\n",
      "            row['Parch'] = int(row['Parch'])\n",
      "            row['Pclass'] = int(row['Pclass'])\n",
      "            row['Embarked'] = row['Embarked'] if row['Embarked'] else 'S'\n",
      "            data.append(row)\n",
      "        if is_training:    \n",
      "            return np.array(data), np.array(labels)\n",
      "        else:\n",
      "            return ids, np.array(data)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import collections\n",
      "import itertools\n",
      "\n",
      "def extend_data(data):\n",
      "    columns = data[0].keys()\n",
      "    \n",
      "    # {column: [possible values], ...}\n",
      "    column_bins = { column: collections.Counter(row[column] for row in data).keys() for column in columns }\n",
      "    \n",
      "    # [(column1, column2), (column1, column3), ...]\n",
      "    combined_columns = [pair for pair in itertools.combinations(column_bins.keys(), 2)]\n",
      "    \n",
      "    extended_data = []\n",
      "    for row in data:\n",
      "        extension = {\"{}_{}\".format(pair[0], pair[1]) : \"{}_{}\".format(row[pair[0]], row[pair[1]]) for pair in combined_columns}\n",
      "        extended_data.append(dict(row, **extension))\n",
      "    \n",
      "    return np.array(extended_data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import StringIO\n",
      "\n",
      "import pydot\n",
      "from IPython.core.display import Image\n",
      "from sklearn.feature_extraction import DictVectorizer\n",
      "from sklearn import tree\n",
      "from sklearn import cross_validation\n",
      "\n",
      "def dtree(training_data, labels):\n",
      "    '''Creates a decision tree from np.array of dicts'''\n",
      "    vectorizer = DictVectorizer()\n",
      "    vectorizer.fit(training_data)\n",
      "    cleaned_training_data = vectorizer.transform(training_data).toarray()\n",
      "    \n",
      "    classifier = tree.DecisionTreeClassifier(min_samples_leaf=5, max_depth=5)\n",
      "    classifier.fit(cleaned_training_data, labels)\n",
      "    \n",
      "    scores = cross_validation.cross_val_score(classifier, cleaned_training_data, labels, cv=5)\n",
      "    print \"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() / 2)\n",
      "    return classifier, vectorizer\n",
      "\n",
      "def graph_dtree(classifier, vectorizer):\n",
      "    \n",
      "    dot_data = StringIO.StringIO() \n",
      "    tree.export_graphviz(classifier, out_file=dot_data, feature_names=vectorizer.get_feature_names()) \n",
      "    graph = pydot.graph_from_dot_data(dot_data.getvalue()) \n",
      "    graph.write_png(\"decision_tree.png\") \n",
      "    print \"Saved to decision_tree.png\" \n",
      "\n",
      "def display_graph(filename='decision_tree.png'):\n",
      "    return Image(filename=filename)\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "ImportError",
       "evalue": "No module named pydot",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-4-6ce1c8423442>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStringIO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpydot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDictVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mImportError\u001b[0m: No module named pydot"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Create classifier from training data ##"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pylab inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Populating the interactive namespace from numpy and matplotlib\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "training_data, labels = load_data('data/train.csv', True)    \n",
      "print training_data[0], labels[0]\n",
      "\n",
      "extended_training_data = extend_data(training_data)\n",
      "print extended_training_data[-1]\n",
      "\n",
      "clf, vec = dtree(extended_training_data, labels)\n",
      "graph_dtree(clf, vec)\n",
      "display_graph()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{'Fare': 1, 'Embarked': 'S', 'Age': 2, 'Parch': 0, 'Pclass': 3, 'Sex': 'male', 'SibSp': 1} 0\n",
        "{'Embarked_Parch': 'Q_0', 'Fare_Age': '1_2', 'Age_SibSp': '2_0', 'Parch_SibSp': '0_0', 'Embarked': 'Q', 'Embarked_Pclass': 'Q_3', 'Age': 2, 'Pclass_Sex': '3_male', 'Parch': 0, 'Pclass': 3, 'Age_Parch': '2_0', 'Pclass_SibSp': '3_0', 'Parch_Sex': '0_male', 'Embarked_SibSp': 'Q_0', 'Parch_Pclass': '0_3', 'Age_Sex': '2_male', 'SibSp': 0, 'Embarked_Age': 'Q_2', 'Fare': 1, 'Fare_Embarked': '1_Q', 'Sex_SibSp': 'male_0', 'Fare_Pclass': '1_3', 'Fare_SibSp': '1_0', 'Fare_Parch': '1_0', 'Age_Pclass': '2_3', 'Sex': 'male', 'Fare_Sex': '1_male', 'Embarked_Sex': 'Q_male'}\n"
       ]
      },
      {
       "ename": "NameError",
       "evalue": "name 'dtree' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-6-bef330ab1089>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mextended_training_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextended_training_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mgraph_dtree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mdisplay_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mNameError\u001b[0m: name 'dtree' is not defined"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Use Classifer to classify test data, and output to csv file ##"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def predict(data, classifier, vectorizer):\n",
      "    cleaned_data = vectorizer.transform(data).toarray()\n",
      "    return classifier.predict(cleaned_data)\n",
      "\n",
      "def write_predictions(id_predictions, filename='prediction.csv'):\n",
      "    with open(filename, 'w') as csvfile:\n",
      "        csvfile.write(\"PassengerId,Survived\\n\")\n",
      "        for p in id_predictions:\n",
      "            csvfile.write(\"{},{}\\n\".format(p[0], p[1]))\n",
      "    print \"Wrote {} lines to {}\".format(len(predictions), filename)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ids, test_data = load_data('data/test.csv', False)\n",
      "extended_test_data = extend_data(test_data)\n",
      "predictions = predict(extended_test_data, clf, vec)\n",
      "id_predictions = zip(ids, predictions)\n",
      "\n",
      "print \"Prediction on first and last 5 test rows\"\n",
      "print predictions[:5]\n",
      "print predictions[-5:]\n",
      "print\n",
      "\n",
      "write_predictions(id_predictions, 'combined_feature_predictions.csv')\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'vec' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-8-80aff283ee16>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/test.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mextended_test_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextend_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextended_test_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mid_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mNameError\u001b[0m: name 'vec' is not defined"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Results ##\n",
      "Unfortunately, combined features did not perform better than their singleton counterparts. I can think of two reasons as to why:\n",
      "\n",
      " * Logically, I think that combined features are the same as an AND statement. For example, with two nodes in a decision tree, you can get a combined feature naturally. Split on Age then Split on gender, is probably about the same as Age_Gender\n",
      " * Doing this required me to mince the data more, by doing more binning, which effectively removes some of the data."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Experimentation Area ###"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "d = {1:11}\n",
      "d2 = dict(d, **{2:22})\n",
      "print d\n",
      "print d2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''\n",
      "Building a function to measure the amount of information in a given sample. \n",
      "I made this up, so it's not necessarily the best approach.\n",
      "'''\n",
      "\n",
      "import math\n",
      "def correlation(labels):\n",
      "    total = len(labels)\n",
      "    lived = sum(int(l) for l in labels)\n",
      "    died = total-lived\n",
      "    try:\n",
      "        return (abs(lived-died)/float(total))\n",
      "    except ZeroDivisionError:\n",
      "        return -1\n",
      "\n",
      "def weighted_correlation(labels):\n",
      "    try:\n",
      "        return correlation(labels) * math.log(len(labels))\n",
      "    except ValueError:\n",
      "        return -1\n",
      "\n",
      "#print correlation(labels)\n",
      "\n",
      "l1 = [0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,1,0,0,0,0,0,0,0,0,0,0,]\n",
      "l2 = [1,1,1,1,1,1,0]\n",
      "print correlation(l1), weighted_correlation(l1)\n",
      "print\n",
      "print correlation(l2), weighted_correlation(l2)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''\n",
      "Testing out the correlation to see which combinations of features give the most information.\n",
      "'''\n",
      "\n",
      "columns = training_data[0].keys()\n",
      "\n",
      "import collections\n",
      "column_bins = { column: collections.Counter(row[column] for row in training_data).keys() for column in columns }\n",
      "\n",
      "def select_labels(data, key_1, value_1, key_2, value_2):\n",
      "    result = []\n",
      "    for i, row in enumerate(data):\n",
      "        if row[key_1] == value_1 and row[key_2] == value_2:\n",
      "            result += labels[i] \n",
      "    return result\n",
      "\n",
      "import itertools\n",
      "second_order_corr = {}\n",
      "for pair in itertools.combinations(column_bins.keys(), 2):\n",
      "    for inner_pair in itertools.product(column_bins[pair[0]], column_bins[pair[1]]):\n",
      "        second_order_corr[\"{}={}, {}={}\".format(pair[0], inner_pair[0], pair[1], inner_pair[1])] = \\\n",
      "            weighted_correlation(select_labels(training_data, pair[0], inner_pair[0], pair[1], inner_pair[1]))\n",
      "    \n",
      "#>>> # dictionary sorted by value\n",
      "#>>> OrderedDict(sorted(d.items(), key=lambda t: t[1]))\n",
      "        \n",
      "second_order_corr = collections.OrderedDict(sorted(second_order_corr.items(), key=lambda t: t[1], reverse=True))\n",
      "    \n",
      "for k, v in second_order_corr.iteritems():\n",
      "    print k, v\n",
      "\n",
      "#print correlation(select_labels(training_data, 'embarked', 'Q', 'age', 1))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}